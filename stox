#!/usr/bin/env python3
# Stox, a prediction engine for financial time series data
# Copyright (C) 2017-2020 Gokalp Ozcan

import pandas as pd
import argparse, datetime, os, sys, psutil
from time import perf_counter
from joblib import dump, load
from sklearn.metrics import mean_absolute_error, explained_variance_score
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, VotingRegressor
from lightgbm import LGBMRegressor
import lib.tickers as ticker_lists
from lib.db import db_engine

BASE_DIR = os.path.dirname(os.path.realpath(__file__))
pd.set_option('mode.chained_assignment', None)

now = datetime.datetime.now()
day_of_week = now.strftime("%a").upper()
day_of_week = 'FRI' if day_of_week in ['SAT', 'SUN'] else day_of_week
TIMESTAMP = now.strftime("%Y-%m-%d-%H-%M-%S")

parser = argparse.ArgumentParser()
parser.add_argument('-m', '--markets', default='AU', help='Comma-separated list of markets. Default : AU')
parser.add_argument('-s', '--split-date', default='2016-01-01', help='Train/Test split date. Default : 2016-01-01')
parser.add_argument('-t', '--size', default=200, help='Number of estimator trees to build. Default: 200.')
parser.add_argument('-r', '--seed', default=6, help='Seed for initialising the model weights with')
parser.add_argument('-v', '--verbose', default=1, help='Integer greater than zero. Greater this number, more info is printed during run. Default: 1.')
parser.add_argument('-b', '--lookback', default=6, help='The number of periods for look-back features. Default: 6.')
parser.add_argument('-f', '--lookfwd', default=1, help='The number of periods into the future to predict at. Default: 1.')
parser.add_argument('-w', '--resample', default=f'W-{day_of_week}', help="Resampling window size. 'no' to turn off resampling, or any pandas-format resampling specification. Default: weekly resampling on current business day.")
parser.add_argument('-a', '--automl', default=False, help='Parameter search through hypopt. Default: disabled', action='store_true')
parser.add_argument('-d', '--dump-data', default=False, help='Dump the datasets, predictions and results into parquet files. Default: False', action='store_true')
parser.add_argument('-l', '--load-data', default=False, help='Load the datasets from the last dump. Default: False', action='store_true')
parser.add_argument('-o', '--load-model', default=False, help='Load a trained model from a file. Default: False', action='store_true')
parser.add_argument('-p', '--predict', default=False, help='Make predictions. Default: False', action='store_true')
parser.add_argument('-e', '--save-predictions', default=False, help='Save predictions on test data to a CSV file. Default: False', action='store_true')
parser.add_argument('-i', '--intraday-predictions', default=False, help='Fetch and make predictions on intraday data. Default: False', action='store_true')

MARKETS = parser.parse_args().markets
SPLIT_DATE = parser.parse_args().split_date
SIZE = int(parser.parse_args().size)
SEED = int(parser.parse_args().seed)
VERBOSE = int(parser.parse_args().verbose)
LOOKBACK = int(parser.parse_args().lookback)
LOOKFWD = int(parser.parse_args().lookfwd)
RESAMPLE = parser.parse_args().resample
AUTOML = parser.parse_args().automl
DUMP_DATA = parser.parse_args().dump_data
LOAD_DATA = parser.parse_args().load_data
LOAD_MODEL = parser.parse_args().load_model
PREDICT = parser.parse_args().predict
SAVE_PREDICTIONS = parser.parse_args().save_predictions
INTRADAY_PREDICTIONS = parser.parse_args().intraday_predictions

FEATURE_SUBSAMPLE = 1.0
MIN_TEST_SAMPLES = 10 # minimum number of test samples required for an individual ticker to bother calculating its alpha and making predictions
STAMP = f"{MARKETS.replace(',', '+')}-{LOOKBACK}-{RESAMPLE}-{LOOKFWD}" # to be used in naming dataset & model dump files
TICKERS = ticker_lists.by_market([ f"'{m}'" for m in MARKETS.split(',')])
model_file = f'{BASE_DIR}/models/{STAMP}.bin'

print('Stox started on', TIMESTAMP, 'for', len(TICKERS), 'tickers in markets', MARKETS)
print('resampling window:', RESAMPLE, 'Lookback:', LOOKBACK, 'Lookforward:', LOOKFWD)

if LOAD_DATA:
    ds_train = load(f'{BASE_DIR}/ds_dumps/ds_train_{STAMP}.bin')
    ds_test  = load(f'{BASE_DIR}/ds_dumps/ds_test_{STAMP}.bin')
else:
    from dataset import DataSet
    ds_train = DataSet(tickers=TICKERS, lookback=LOOKBACK, lookfwd=LOOKFWD, predicate=f"date < '{SPLIT_DATE}'" , resample=RESAMPLE).data
    ds_test  = DataSet(tickers=TICKERS, lookback=LOOKBACK, lookfwd=LOOKFWD, predicate=f"date >= '{SPLIT_DATE}'", resample=RESAMPLE, keep_predictors=True, intraday=INTRADAY_PREDICTIONS).data

if VERBOSE > 0:
    print('\n--------------------------- Train dataset ---------------------------')
    print(ds_train.describe())
    print(ds_train.info(memory_usage='deep'))
    print('\n--------------------------- Test dataset ----------------------------')
    print(ds_test.describe())
    print(ds_test.info(memory_usage='deep'))

if DUMP_DATA:
    dump(ds_train, f'{BASE_DIR}/ds_dumps/ds_train_{STAMP}.bin', compress=True)
    dump(ds_test , f'{BASE_DIR}/ds_dumps/ds_test_{STAMP}.bin', compress=True)

features = [ f for f in list(ds_train.columns) if f.startswith('f_') ]

X_train = ds_train[features]
y_train = ds_train['future']

X_test = ds_test[features]
y_test = ds_test['future']

predictors = X_test[y_test.isnull()]
predictors_latest = predictors.index.get_level_values('date').max()
X_test.drop(predictors.index, inplace=True)
y_test.drop(predictors.index, inplace=True)

train_samples, test_samples, total_samples = len(X_train), len(X_test), len(X_train) + len(X_test)
print(  'X_train:', X_train.shape, 'X_test:', X_test.shape,
        'y_train:', y_train.shape, 'y_test:', y_test.shape,
        'predictors:', predictors.shape, '-',
        round(100 * train_samples / total_samples), '/', round(100 * test_samples / total_samples), '% split')

LGB_params = {  'boosting_type': 'gbdt', 'colsample_bytree': 0.6,
                'min_child_samples': 20, 'min_child_weight': 0.001,
                'num_leaves': 63, 'learning_rate': 0.1,
                'reg_alpha': 0.001, 'reg_lambda': 1, 'objective': 'mae' }

RFR_params = {  'max_features': 0.6, 'criterion': 'mse', # !
                'min_samples_leaf': 20, 'min_samples_split': 2 }

common_params = { 'n_estimators': SIZE, 'random_state': SEED, 'verbose': VERBOSE, 'n_jobs': -1 }

if LOAD_MODEL:
    model = load(model_file)
    print('Model loaded from', model_file)
else:
    model = VotingRegressor(estimators=[    ('lgb', LGBMRegressor(          **LGB_params, **common_params)),
                                            # ('rfr', ExtraTreesRegressor(    **RFR_params, **common_params))
                            ])

if VERBOSE > 1:
    print(model)

time_start_tr = perf_counter()

if AUTOML:
    from hypopt import GridSearch
    os.nice(19)

    param_grid = [{
        # --- LGBMRegressor options ---
        'lgb__objective': [ 'mae' ],
        # 'lgb__boosting_type': [ 'gbdt', 'dart', 'goss' ], # 'rf' was broken
        'lgb__num_leaves': [ 31, 47, 63, 95, 111, 127 ],
        'lgb__learning_rate': [ .01, .02, .03, .04, .05, .07, .10, .25 ],
        # 'lgb__subsample_for_bin': [ 200000, 500000 ],
        'lgb__min_child_samples': [ 13, 15, 17, 20, 23, 25, 28, 33 ],
        'lgb__min_child_weight': [ 0.001, 0.01, 0.1 ],
        'lgb__min_split_gain': [ 0.0, 0.001, 0.01 ],
        'lgb__colsample_bytree': [ .45, .52, .60, .69, 0.80 ],
        'lgb__reg_alpha': [ 0.001, 0.01, 0.1 ],
        'lgb__reg_lambda': [ 0.01, 0.1, 1],
        'lgb__verbose': [ 0 ],

        # --- RandomForestRegressor / ExtraTreesRegressor options ---
        # 'rfr__max_features': [ .45, .52, .60, .69, 0.80 ],
        # 'rfr__min_samples_leaf': [ 15, 17, 19, 21, 23 ],
        # 'rfr__min_samples_split': [ 2, 3 ],
        # 'rfr__max_depth': [ 15, 20, 25 ],
        # 'rfr__n_estimators': [ int(common_params['n_estimators'] / 2) ],
        # 'rfr__verbose': [ 0 ],

        # --- VotingRegressor options ---
        # 'weights':  [ [1/2, 1/2], [2/3, 1/3], [1/3, 2/3] ],
    }]
    model = GridSearch(model=model, param_grid=param_grid, parallelize=False, seed=SEED)
    model.fit(X_train, y_train, X_test, y_test, scoring='neg_mean_absolute_error', verbose=True)

    print('BEST PARAMETERS:', model.get_best_params(), sep='\n')
    model = model.best_estimator_

if not LOAD_MODEL:
    model.fit(X_train, y_train)

if VERBOSE > 0 or AUTOML:
    print('Training took', round(perf_counter() - time_start_tr, 2), 'seconds')

if AUTOML:
    dump(model, model_file, compress=True)
    print('Model saved to', model_file)

if hasattr(model, 'feature_importances_') and (VERBOSE > 0 or FEATURE_SUBSAMPLE < 1.0):
    fi = pd.DataFrame(model.feature_importances_, index=features, columns=['importance']).sort_values('importance', ascending=False)
    print(fi)
    if FEATURE_SUBSAMPLE < 1.0:
        important_features = list(fi.head(round(len(features) * FEATURE_SUBSAMPLE)).index)
        if VERBOSE > 1:
            print('important features:', important_features, f'({len(important_features)})')
        unimportant_features = [ f for f in features if f not in important_features]
        
        X_train.drop(unimportant_features, axis=1, inplace=True)
        X_test.drop(unimportant_features, axis=1, inplace=True)
        predictors.drop(unimportant_features, axis=1, inplace=True)

        model = LGBMRegressor(**LGB_params, **common_params)
        model.fit(X_train, y_train) # fit a new model on reduced set of features

if PREDICT:
    results = pd.DataFrame()
    for t, p in predictors.groupby(level=1):
        if p.index.get_level_values('date').values[0] != predictors_latest:
            if VERBOSE > 0:
                print('Not predicting', t, 'as its predictor is out-of-date.')
            continue

        if p.isnull().values.any():
            if VERBOSE > 0:
                print('Not predicting', t, 'as it has NaN in its predictor.')
            continue

        try:
            xT, yT = X_test.xs(t, level=1, drop_level=False), y_test.xs(t, level=1, drop_level=False)
        except KeyError:
            continue

        if len(yT) < MIN_TEST_SAMPLES:
            if VERBOSE > 0:
                print('Not predicting', t, 'as it has less than', MIN_TEST_SAMPLES, 'samples.')
            continue

        predictions = model.predict(xT)
        results.at[t, 'predicted_at'] = p.index[0][0]
        results.at[t, 'prediction'] = model.predict(p)[0]
        results.at[t, 'volatility'] = abs(yT).mean()
        results.at[t, 'MAE'] = mean_absolute_error(yT, predictions)
        results.at[t, 'alpha'] = (results.loc[t, 'volatility'] / results.loc[t, 'MAE'] - 1) * 100
        results.at[t, 'var_score'] = explained_variance_score(yT, predictions)
        results.at[t, 'test_samples'] = xT.shape[0]
        results.at[t, 'potential'] = results.loc[t, 'prediction'] * \
                                    (results.loc[t, 'var_score'] if results.loc[t, 'var_score'] > 0 else 0) * \
                                    (results.loc[t, 'alpha'] if results.loc[t, 'alpha'] > 0 else 0)

    results.index.rename('ticker', inplace=True)
    results = results.sort_values('potential', ascending=False).round(2)
    print(results)
    if VERBOSE > 0:
        print(results.describe())
    results.to_csv(f'{BASE_DIR}/results/{TIMESTAMP}.csv')
    results.reset_index().to_sql(STAMP, if_exists='replace', schema='results', index=False, con=db_engine())

# Calculate and print prediction results on the test data
predictions_on_test = model.predict(X_test)
volatility_on_test = round(abs(y_test).mean(), 4)
error_on_test = round(mean_absolute_error(y_test, predictions_on_test), 4)
print('Overall volatility:', volatility_on_test, ', error:', error_on_test, ', alpha:', round((volatility_on_test / error_on_test - 1) * 100, 2))

if SAVE_PREDICTIONS:
    y_test = pd.concat([y_test, pd.DataFrame(predictions_on_test, index=y_test.index)], axis=1)
    y_test.columns = [*y_test.columns[:-1], 'prediction']
    y_test.reset_index().to_sql(STAMP, if_exists='replace', schema='predictions', index=False, con=db_engine())

if VERBOSE > 0:
    process = psutil.Process(os.getpid())
    print('memory used:', round(process.memory_info().rss / (2 ** 30), 1), 'GB')
